# Key Design Decisions - RAG Cybersecurity KB

Dokumentation der wichtigsten technischen und strategischen Entscheidungen beim Bau dieses 
RAG-Systems.

**Kontext:** Diese Entscheidungen wurden wÃ¤hrend der Entwicklung getroffen und reflektieren 
Trade-offs zwischen QualitÃ¤t, Kosten, KomplexitÃ¤t und Time-to-Market.

---

## ğŸ¯ Decision 1: FAISS vs. Pinecone (Vector Store)

### Context
BenÃ¶tigte einen Vector Store fÃ¼r Embedding-basierte Suche Ã¼ber Cybersecurity-Dokumente.

### Options Considered
1. **FAISS** (Facebook AI Similarity Search)
   - Local, kostenlos
   - Installation: `pip install faiss-cpu`
   - Keine API-Limits

2. **Pinecone**
   - Cloud-basiert
   - Skaliert automatisch
   - $70+/Monat fÃ¼r Production

3. **Chroma**
   - Hybrid (lokal + Cloud)
   - Einfache API
   - Noch relativ neu

### Decision
**GewÃ¤hlt: FAISS**

### Reasoning
- **Kosten:** Komplett kostenlos fÃ¼r Entwicklung und Demo
- **Ausreichend:** FÃ¼r < 100 Dokumente performt FAISS exzellent
- **Kontrolle:** Lokale Daten, keine Cloud-AbhÃ¤ngigkeit
- **Learning:** Verstehe Low-Level Vector Search besser

### Trade-offs
**âœ… Pro:**
- Keine API-Kosten
- Volle Kontrolle
- Einfaches Setup fÃ¼r Portfolio-Projekt

**âŒ Contra:**
- Skaliert nicht gut (> 10k Dokumente)
- Kein Auto-Scaling
- Kein Built-in Monitoring

### When to reconsider
FÃ¼r Production mit > 1000 Dokumenten oder Multi-User-System â†’ Pinecone oder Weaviate besser 
geeignet.

**Status:** âœ… Richtige Wahl fÃ¼r dieses Projekt

---

## ğŸ¯ Decision 2: Chunk Size & Overlap

### Context
Initiales Setup: 500 Tokens/Chunk, kein Overlap â†’ Distance-Werte >1.5, schlechte 
Retrieval-QualitÃ¤t.

### Options Considered
1. **500 Tokens, kein Overlap** (Initial)
   - 7 Chunks total
   - Problem: Wichtige Infos "zerschnitten"

2. **300 Tokens, kein Overlap**
   - Mehr Chunks (ca. 11)
   - Risiko: Kontext-Verlust an Grenzen

3. **300 Tokens + 50 Token Overlap** âœ…
   - 13 Chunks
   - Overlap verhindert Informationsverlust

4. **200 Tokens + 100 Overlap**
   - Maximale GranularitÃ¤t
   - Aber: Zu viele redundante Chunks

### Decision
**GewÃ¤hlt: 300 Tokens + 50 Overlap**

### Reasoning
- **Empirisch getestet:** Distance-Verbesserung 10-19%
- **Balance:** Genug GranularitÃ¤t ohne Redundanz
- **Best Practice:** Industry-Standard fÃ¼r RAG-Systeme

### Results (Before/After)
| Query | Before (500) | After (300+50) | Improvement |
|-------|--------------|----------------|-------------|
| VPN | 1.54 ğŸŸ¡ | 1.25 ğŸŸ¡ | **19%** âœ… |
| Phishing | 0.96 ğŸŸ¢ | 0.86 ğŸŸ¢ | **10%** âœ… |
| MFA | 1.35 ğŸŸ¡ | 1.31 ğŸŸ¡ | 3% |

### Trade-offs
**âœ… Pro:**
- Bessere Retrieval-QualitÃ¤t
- Keine Edge-Case-Verluste
- Mehr Kontext pro Chunk

**âŒ Contra:**
- Mehr API-Calls fÃ¼r Embeddings (einmalig)
- Leicht hÃ¶here Latenz (0.2s â†’ 0.3s)

**Status:** âœ… Validiert durch Tests

---

## ğŸ¯ Decision 3: GPT-4o-mini vs. GPT-4o (LLM)

### Context
Brauchte LLM fÃ¼r Antwort-Generierung nach Retrieval.

### Options Considered
1. **GPT-4o**
   - HÃ¶chste QualitÃ¤t
   - $10/1M input tokens
   - Overkill fÃ¼r strukturierte Antworten?

2. **GPT-4o-mini** âœ…
   - Gute QualitÃ¤t
   - $0.15/1M input tokens (~70x gÃ¼nstiger!)
   - Optimiert fÃ¼r Chat/QA

3. **Claude Sonnet 3.5**
   - Ã„hnliche QualitÃ¤t wie GPT-4o
   - Noch teurer
   - Redundant mit OpenAI-Stack

### Decision
**GewÃ¤hlt: GPT-4o-mini**

### Reasoning
- **Kosten:** Bei 1000 Queries/Monat: ~$5 statt ~$50
- **QualitÃ¤t:** FÃ¼r RAG-QA mit klarem Context = ausreichend
- **Latenz:** Schneller als GPT-4o (0.8s vs. 1.5s)

### Test Results
Paralleltests mit 10 Queries:
- GPT-4o: Perfekte Antworten, aber keine merkliche Verbesserung
- GPT-4o-mini: 9/10 perfekt, 1/10 leicht unschÃ¤rfer (akzeptabel)

### Trade-offs
**âœ… Pro:**
- Massive Kosteneinsparung
- Schnellere Responses
- Ausreichend fÃ¼r Business-Case

**âŒ Contra:**
- Bei sehr komplexen Queries minimal schlechter
- Weniger "kreativ" bei Edge-Cases

### When to reconsider
Wenn User-Feedback zeigt: "Antworten zu oberflÃ¤chlich" â†’ Upgrade zu GPT-4o.

**Status:** âœ… Optimal fÃ¼r Cost/Quality-Balance

---

## ğŸ¯ Decision 4: Prompt Pattern "Cite Sources"

### Context
Musste Prompt-Pattern wÃ¤hlen fÃ¼r Antwort-Generierung. Ziel: Balance zwischen Hilfsbereitschaft und 
Transparenz.

### Options Considered
**Pattern A: "Answer ONLY from context"**
```python
"If information not in context, say 'Keine Info verfÃ¼gbar.'"
```
- âœ… Pro: Keine Halluzinationen
- âŒ Contra: Oft zu restriktiv, unhilfreich

**Pattern B: "Context + General Knowledge"**
```python
"Use context primarily, supplement with knowledge if needed."
```
- âœ… Pro: VollstÃ¤ndigere Antworten
- âŒ Contra: Schwer zu tracen, was aus Docs vs. Modell-Wissen

**Pattern C: "Cite Sources explicitly"** âœ…
```python
"Answer based on context. Cite sources in brackets: [file.md]"
```
- âœ… Pro: Transparent, nachvollziehbar
- âœ… Pro: LLM bleibt context-focused
- âš ï¸ Neutral: Etwas verbose

### Decision
**GewÃ¤hlt: Pattern C (Cite Sources)**

### Reasoning
- **Transparenz:** User sieht, woher Info kommt
- **Vertrauen:** Explizite Quellen â†’ hÃ¶here GlaubwÃ¼rdigkeit
- **Debugging:** Bei falscher Antwort â†’ Check Source direkt
- **Business-fit:** Ideal fÃ¼r Customer Support, Compliance

### Test Results (Query: "Welcher ist der beste Password-Manager?")
- Pattern A: "Keine Info verfÃ¼gbar" (technisch korrekt, aber unhilfreich)
- Pattern B: Suggestive Empfehlung ohne Quellenangabe
- Pattern C: Liste + Disclaimer + Sources (âœ… beste Balance)

### Trade-offs
**âœ… Pro:**
- VertrauenswÃ¼rdigkeit
- Nachvollziehbarkeit
- Professioneller Eindruck

**âŒ Contra:**
- Antworten etwas lÃ¤nger
- Citations manchmal redundant bei offensichtlichen Fakten

**Status:** âœ… Beste Wahl fÃ¼r Business-Use-Case

---

## ğŸ¯ Decision 5: "One Document = One Major Topic"

### Context
Musste entscheiden: Wenige groÃŸe Dokumente oder viele kleine, fokussierte Dokumente?

### Options Considered
1. **Ein groÃŸes "Cybersecurity_Guide.md"**
   - Alle Topics in einem Dokument
   - Einfacher zu maintainen (1 File)
   - Problem: Chunks mischen Topics

2. **Ein Dokument pro Kategorie**
   - `passwords.md`, `network.md`, `threats.md`
   - Moderat granular
   - Problem: MFA in Passwords oder Network?

3. **Ein Dokument pro Major Topic** âœ…
   - `phishing_detection.md`, `vpn_guide.md`, `mfa_setup.md`
   - Maximum Focus
   - Problem: Mehr Files zu managen

### Decision
**GewÃ¤hlt: One Document = One Major Topic**

### Reasoning
**Empirische Beobachtung:**
- Phishing (eigenes Doc) â†’ Distance 0.86 ğŸŸ¢
- MFA (Unter-Topic) â†’ Distance 1.31 ğŸŸ¡

**Warum?**
- Chunks bleiben thematisch konsistent
- Keine Topic-Vermischung
- Updates isoliert (Ã¤ndere VPN-Doc ohne Passwords zu touchen)
- Bessere Semantic Separation

### Implementation
```
docs/
â”œâ”€ phishing_detection.md      â† Haupt-Thema
â”œâ”€ password_security.md        â† Haupt-Thema
â”œâ”€ network_security.md         â† EnthÃ¤lt VPN + Firewalls
â””â”€ (future) mfa_guide.md       â† MFA auslagern
```

### Trade-offs
**âœ… Pro:**
- Bessere Retrieval-QualitÃ¤t
- Klarere Verantwortlichkeiten (wer updated was?)
- Einfacher zu erweitern

**âŒ Contra:**
- Mehr Dateien zu verwalten
- Redundanz mÃ¶glich (z.B. "Passwords" in Phishing + Password-Doc)

**Status:** âœ… Validiert durch Distance-Improvements

---

## ğŸ¯ Decision 6: Top-K = 3 (Retrieval Strategy)

### Context
Wie viele Chunks sollten fÃ¼r jede Query retrieved werden?

### Options Considered
1. **Top-K = 1**
   - Schnell, gÃ¼nstig
   - Risiko: Bester Chunk kÃ¶nnte falsch sein

2. **Top-K = 3** âœ…
   - Standard in RAG-Systemen
   - Balance zwischen Kontext und Redundanz

3. **Top-K = 5**
   - Mehr Kontext
   - Aber: Oft redundant oder irrelevant

4. **Distance-Threshold (adaptive)**
   - Nur Chunks mit Distance <1.5
   - Problem: Manchmal kein Chunk qualifiziert

### Decision
**GewÃ¤hlt: Top-K = 3 (fixed)**

### Reasoning
- **Standard:** Most RAG implementations use 3-5
- **Balance:** Genug fÃ¼r Multi-Paragraph-Antworten
- **Token-Budget:** 3 Chunks Ã— 300 Tokens = 900 Tokens Context (reasonable)

### Observed Patterns
**Phishing-Query:**
```
Top-1: 0.86 âœ…
Top-2: 0.93 âœ…
Top-3: 1.45 âš ï¸
â†’ 2/3 relevant, 3rd redundant
```

**MFA-Query:**
```
Top-1: 1.31 âš ï¸
Top-2: 1.41 âš ï¸
Top-3: 1.49 âš ï¸
â†’ Alle ok, aber keiner exzellent
```

### Trade-offs
**âœ… Pro:**
- Robustheit (falls Top-1 suboptimal)
- Multi-Source-Antworten mÃ¶glich

**âŒ Contra:**
- Top-3 oft wenig relevant
- Verschwendet Tokens

### Future Optimization
ErwÃ¤ge fÃ¼r v3.0:
```python
# Adaptive Top-K basierend auf Top-1 Distance
if top1_distance < 1.0:
    k = 2  # Top-1 ist perfekt, brauche nur 1-2 mehr
else:
    k = 5  # Top-1 schwach, hole mehr Optionen
```

**Status:** âœ… Funktioniert, aber optimierbar

---

## ğŸ¯ Decision 7: Python 3.12 vs. 3.10

### Context
Musste Python-Version wÃ¤hlen fÃ¼r Development.

### Problem Encountered
Initial: Python 3.14 (Beta) â†’ Incompatible mit vielen Packages

### Decision
**GewÃ¤hlt: Python 3.12** (Downgrade von 3.14)

### Reasoning
- **KompatibilitÃ¤t:** Alle Dependencies (FAISS, OpenAI SDK) stable
- **Long-term:** 3.12 LTS bis 2028
- **Performance:** Marginal schneller als 3.10

### Trade-offs
**âœ… Pro:**
- Stabil, Production-ready
- Community-Support

**âŒ Contra:**
- Verpasste neue Features aus 3.13+

**Status:** âœ… Pragmatische Wahl

---

## ğŸ“Š Decision Summary Matrix

| Decision | Impact | Confidence | Would Change? |
|----------|--------|------------|---------------|
| FAISS vs Pinecone | High | High | âŒ No |
| 300+50 Chunking | High | High | âŒ No |
| GPT-4o-mini | High | High | âŒ No |
| Cite Sources Pattern | High | High | âŒ No |
| One Doc = One Topic | High | High | âŒ No |
| Top-K = 3 | Medium | Medium | âš ï¸ Maybe (adaptive) |
| Python 3.12 | Low | High | âŒ No |

---

## ğŸ”„ Decisions to Revisit for v3.0

### 1. Hybrid Search (Semantic + BM25)
**Why:** Akronyme/Fachbegriffe schlecht mit Semantic Search
**Expected Impact:** +15% fÃ¼r Technical Queries

### 2. Reranking (Cross-Encoder)
**Why:** Top-K enthÃ¤lt oft irrelevante Chunks
**Expected Impact:** +10% Average Relevance

### 3. Adaptive Top-K
**Why:** Fixed K = 3 suboptimal fÃ¼r verschiedene Query-Types
**Expected Impact:** -20% Token-Waste

---

## ğŸ“ Meta-Learning: Decision-Making Process

**Pattern, der funktioniert hat:**
1. **Measure First:** Distance-Werte als Baseline
2. **Hypothesis:** "Kleinere Chunks = bessere Distance"
3. **Test:** 500 â†’ 300 Tokens
4. **Validate:** 10-19% Improvement
5. **Document:** In diesem File

**Lessons Learned:**
- Empirisches Testen > Vermutungen
- Distance-Werte = objektive Metrik
- Trade-offs explizit machen (Cost vs. Quality)
- "Good enough" besser als "perfekt aber nie fertig"

---

**Created:** [Datum]  
**Last Updated:** [Datum]  
**Version:** 1.0
